{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "#In order to make the labels simpler when doing analysis, both before and after imputting the data to the model, the mapping to the simple labels is done here. This is the same mapping that was also used for the REBEL model\n",
    "relations_mapper = {'/people/person/nationality': 'country of citizenship', '/sports/sports_team/location': 'headquarters location', \n",
    "            '/location/country/administrative_divisions': 'contains administrative territorial entity', '/business/company/major_shareholders': 'shareholders', \n",
    "            '/people/ethnicity/people': 'country of origin', '/people/ethnicity/geographic_distribution': 'denonym', \n",
    "            '/business/company_shareholder/major_shareholder_of': 'major shareholder', '/location/location/contains': 'location',\n",
    "            '/business/company/founders': 'founded by', '/business/person/company': 'employer', '/business/company/advisors': 'advisors', \n",
    "            '/people/deceased_person/place_of_death': 'place of death', '/business/company/industry': 'industry', \n",
    "            '/people/person/ethnicity': 'ethnicity', '/people/person/place_of_birth': 'place of birth', \n",
    "            '/location/administrative_division/country': 'country', '/people/person/place_lived': 'residence', \n",
    "            '/sports/sports_team_location/teams': 'member of sports team', '/people/person/children': 'child', \n",
    "            '/people/person/religion': 'religion', '/location/neighborhood/neighborhood_of': 'neighborhood of', \n",
    "            '/location/country/capital': 'capital', '/business/company/place_founded': 'location of formation', \n",
    "            '/people/person/profession': 'occupation'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script that does everything in the conversion to the sequence labeling task (in MaChamp format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  564\n",
      "test :  55\n",
      "dev :  55\n"
     ]
    }
   ],
   "source": [
    "def get_concatenations(the_list_of_lists,concatenation_type,seperator=None):\n",
    "    #this script takes all the lists of an instance (if there are more than one relation) and concatenates these \n",
    "    #(e.g. [o,o,likes_person:-1] and [friends_with:2,o,o] become [friends_with:2,o,likes_person:-1]\n",
    "    #or [o,o,like_person:-1] and [o,o,lives_in:-2] become [o,o,like_person:-1|lives_in:-2])\n",
    "\n",
    "    #To make the concatenation easier, a list of lists for each token will be made, such that all the labels a specific token would have gets placed into the same inner list.\n",
    "    #This means, for the next bit of code that we can easily see which labels should be concatenated, and which token they would belong to\n",
    "\n",
    "    zipper_stuff = [[] for _ in range(len(the_list_of_lists[0]))]\n",
    "    for list_inst in the_list_of_lists:\n",
    "        for i,el in enumerate(list_inst):\n",
    "            zipper_stuff[i].append(el)\n",
    "        \n",
    "    output_stuff = []\n",
    "\n",
    "    #In case we are looking at relation labels, if there are only 'o' labels for a token, we just append a single 'o' label; \n",
    "    #if there is an 'o' label and something else, we ignore the 'o' label alltogether; \n",
    "    #if there are more than two non-'o' labels, we concatenate these using the specified seperator\n",
    "\n",
    "    if concatenation_type=='relations':\n",
    "        for lst in zipper_stuff:\n",
    "            first_element=True\n",
    "            s = ''\n",
    "            for el in lst:\n",
    "                if not el=='o':\n",
    "                    if first_element:\n",
    "                        s+=el\n",
    "                        first_element=False\n",
    "                    else:\n",
    "                        s+=seperator+el\n",
    "            if len(s)==0:\n",
    "                output_stuff.append('o')\n",
    "            else:\n",
    "                output_stuff.append(s)\n",
    "    \n",
    "    #For entities, since a check to remove nested entities is done earlier, it is assumed there will only be one label per token\n",
    "\n",
    "    if concatenation_type=='entities':\n",
    "        for lst in zipper_stuff:\n",
    "            if len(set(lst))==2:  #This is done in case there is an 'o' and something else for this token. In that case it is the non-'o' label that should be concatenated\n",
    "                for el in lst:\n",
    "                    if not el=='o':\n",
    "                        output_stuff.append(el)\n",
    "                        break\n",
    "            else:\n",
    "                output_stuff.append(lst[0])\n",
    "    return output_stuff\n",
    "\n",
    "\n",
    "#since I did not do the solution with overlapping entities, there should not be problems with the len-set-2 part (unless they are part of different relations)\n",
    "\n",
    "\n",
    "def has_nested_entities(relations_list,len_token_list):    #Checks for nested entities for an instance. Another check was made that showed there were no B-B nested entities (where there are two entities that start at the same token, but one is longer than the other). This check can be found in the next cell\n",
    "\n",
    "    the_checker = ['o' for _ in range(len_token_list)]\n",
    "\n",
    "    for rel_inst in relations_list:\n",
    "        #We make these lists so we can easily compare its entries to the window in 'the_checker' (the full checker)\n",
    "        tmp_checker_head = [None for _ in range(rel_inst[1]-rel_inst[0])]\n",
    "        tmp_checker_tail = [None for _ in range(rel_inst[5]-rel_inst[4])]\n",
    "\n",
    "        tmp_checker_head[0] = 'B-'+rel_inst[2]\n",
    "        tmp_checker_tail[0] = 'B-'+rel_inst[6]\n",
    "\n",
    "        for i in range(1,rel_inst[1]-rel_inst[0]):\n",
    "            tmp_checker_head[i]='I-'+rel_inst[2]\n",
    "\n",
    "        for i in range(1,rel_inst[5]-rel_inst[4]):\n",
    "            tmp_checker_tail[i]='I-'+rel_inst[6]\n",
    "\n",
    "        # If it finds something where the head entity should be that is not the same entity and is npt just 'o's        \n",
    "        if ((not tmp_checker_head==the_checker[rel_inst[0]:rel_inst[1]]) and (not set(the_checker[rel_inst[0]:rel_inst[1]])==set(['o']))):\n",
    "            return True\n",
    "\n",
    "        #Stored so we can check for nested entities across relations\n",
    "        else: \n",
    "            the_checker[rel_inst[0]:rel_inst[1]]=tmp_checker_head\n",
    "\n",
    "\n",
    "        #the same thing as above but for tail\n",
    "        if ((not tmp_checker_tail==the_checker[rel_inst[4]:rel_inst[5]]) and (not set(the_checker[rel_inst[4]:rel_inst[5]])==set(['o']))):\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            the_checker[rel_inst[4]:rel_inst[5]]=tmp_checker_tail\n",
    "\n",
    "    return False    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessor(data_to_get,seperator,savepath):\n",
    "    \n",
    "    #the main script that does most of the work\n",
    "\n",
    "    with open('nyt/{}.json'.format(data_to_get)) as f:\n",
    "        nyt_data=json.load(f)\n",
    "    \n",
    "    all_relation_labels = []\n",
    "    all_entity_labels = []\n",
    "    all_tokens = []\n",
    "\n",
    "    nested_entities_counter = 0    #Analysis was done to find the number of nested entities\n",
    "\n",
    "    for inst in nyt_data:\n",
    "        relation_labels = []\n",
    "        entity_labels = []\n",
    "\n",
    "        if has_nested_entities(inst['spo_details'],len(inst['tokens'])): #checking for nested entities, and discarding the instance if it has it\n",
    "            nested_entities_counter+=1\n",
    "            continue\n",
    "        else:\n",
    "            for el in inst['spo_details']:\n",
    "                #A list is made for each relation, containing the entities in it\n",
    "                ent = ['o' for _ in range(len(inst['tokens']))]\n",
    "\n",
    "                ent[el[0]]='B-'+el[2]\n",
    "                ent[el[4]]='B-'+el[6]\n",
    "\n",
    "                for i in range(el[4]+1,el[5]):\n",
    "                    ent[i]='I-'+el[6]\n",
    "\n",
    "                for i in range(el[0]+1,el[1]):\n",
    "                    ent[i]='I-'+el[2]\n",
    "\n",
    "                entity_labels.append(ent)\n",
    "\n",
    "        \n",
    "\n",
    "            concatted_entity_list = get_concatenations(entity_labels,'entities')  #the entities for all relations in the instance are concatenated\n",
    "\n",
    "            #In order to get the relational index, a dictionary with the start index of each entity is made\n",
    "            entities_looker_upper = dict()\n",
    "            i=0\n",
    "            for idx,el in enumerate(concatted_entity_list):\n",
    "                if el[0]=='B':\n",
    "                    entities_looker_upper[idx]=i\n",
    "                    i+=1\n",
    "\n",
    "\n",
    "            for el in inst['spo_details']:\n",
    "                rel = ['o' for _ in range(len(inst['tokens']))]\n",
    "\n",
    "                #From this it can be seen how the relational index is made, taking the index from the list of entities of the head entity in the relation, and subtracting it for the index for the tail entity. \n",
    "                #Remember that the arrows go in the opposite direction (from tail to head) due to fewer clashes\n",
    "                rel[el[4]]='B-'+relations_mapper[el[3]]+':'+str(entities_looker_upper[el[0]]-entities_looker_upper[el[4]])  \n",
    "\n",
    "                for i in range(el[4]+1,el[5]):\n",
    "                    rel[i]='I-'+relations_mapper[el[3]]+':'+str(entities_looker_upper[el[0]]-entities_looker_upper[el[4]])\n",
    "\n",
    "                relation_labels.append(rel)\n",
    "\n",
    "\n",
    "            all_relation_labels.append(get_concatenations(relation_labels,'relations',seperator))\n",
    "            all_entity_labels.append(concatted_entity_list)\n",
    "            all_tokens.append(inst['tokens'])\n",
    "            \n",
    "\n",
    "    print(data_to_get,': ',nested_entities_counter)\n",
    "    \n",
    "    #Here the contents of the lists are outputted in the MaChamp format, line by line\n",
    "    with open(savepath+\"{}\".format(data_to_get),'w') as outfile:\n",
    "        for inst_tok, inst_rel,inst_ent in zip (all_tokens,all_relation_labels,all_entity_labels):\n",
    "            for tok,rel,ent in zip(inst_tok,inst_rel,inst_ent):\n",
    "                outfile.write('{}\\t{}\\t{}\\n'.format(tok,rel,ent))\n",
    "            outfile.write('\\n') #an empty line to indicate a new instance\n",
    "\n",
    "\n",
    "data = 'train'\n",
    "preprocessor(data,'|',\"seq_lab_data/new_pipe_sep/nyt.\")\n",
    "\n",
    "data = 'test'\n",
    "preprocessor(data,'|',\"seq_lab_data/new_pipe_sep/nyt.\")\n",
    "\n",
    "data = 'dev'\n",
    "preprocessor(data,'|',\"seq_lab_data/new_pipe_sep/nyt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below is a check to see if there existed B-B entities (where there are two entities that start at the same token, but either one has a different type than the other, or one entity extends the other, or both) in any of the three datasets. As this did not return anything for any of the three datasets, some extra checks in the 'has_nested_entities' method could be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if B-B cases exist\n",
    "\n",
    "data_to_get='test'\n",
    "\n",
    "with open('nyt/{}.json'.format(data_to_get)) as f:\n",
    "        nyt_data=json.load(f)\n",
    "\n",
    "for inst in nyt_data:\n",
    "    types_set = set()\n",
    "    start_idxs_set = set()\n",
    "    entity_position_set = set()\n",
    "    for rel_inst in inst['spo_details']:\n",
    "        #The former two of these checks sees if there are two entities that start at the same token but have different types\n",
    "        #The latter two of these checks sees if there are cases where one entitiy extends another\n",
    "        if (not str(rel_inst[0])+'-'+rel_inst[2] in types_set) and (str(rel_inst[0]) in start_idxs_set) and (not (rel_inst[0],rel_inst[1]) in entity_position_set):\n",
    "            print(True)\n",
    "        start_idxs_set.add(rel_inst[0])\n",
    "        types_set.add(str(rel_inst[0])+'-'+rel_inst[2])\n",
    "        entity_position_set.add((rel_inst[0],rel_inst[1]))\n",
    "\n",
    "        #The same check but for tail\n",
    "        if (not str(rel_inst[4])+'-'+rel_inst[6] in types_set) and (str(rel_inst[4]) in start_idxs_set) and (not (rel_inst[4],rel_inst[5]) in entity_position_set):\n",
    "            print(True)\n",
    "        start_idxs_set.add(rel_inst[4])\n",
    "        types_set.add(str(rel_inst[4])+'-'+rel_inst[6])\n",
    "        entity_position_set.add((rel_inst[4],rel_inst[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test to see if using blanks ('_') instead of labels (thus avoiding the nested entity problem) for the datasets the model was evaluated on would perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative way to do the dev and test sets (without labels)\n",
    "\n",
    "def preprocessor(data_to_get,savepath):\n",
    "    \n",
    "    with open('nyt/{}.json'.format(data_to_get)) as f:\n",
    "        nyt_data=json.load(f)\n",
    "    \n",
    "    all_relation_labels = []\n",
    "    all_entity_labels = []\n",
    "    all_tokens = []\n",
    "\n",
    "\n",
    "    for inst in nyt_data:\n",
    "        \n",
    "        all_relation_labels.append(['_' for _ in range(len(inst['tokens']))])\n",
    "        all_entity_labels.append(['_' for _ in range(len(inst['tokens']))])\n",
    "\n",
    "        all_tokens.append(inst['tokens'])\n",
    "            \n",
    "\n",
    "\n",
    "    with open(savepath+\"{}\".format(data_to_get),'w') as outfile:\n",
    "        for inst_tok, inst_rel,inst_ent in zip (all_tokens,all_relation_labels,all_entity_labels):\n",
    "            for tok,rel,ent in zip(inst_tok,inst_rel,inst_ent):\n",
    "                outfile.write('{}\\t{}\\t{}\\n'.format(tok,rel,ent))\n",
    "            outfile.write('\\n')\n",
    "\n",
    "\n",
    "data = 'test'\n",
    "preprocessor(data,\"seq_lab_data/new_pipe_sep_unlabeled/nyt.\")\n",
    "\n",
    "data = 'dev'\n",
    "preprocessor(data,\"seq_lab_data/new_pipe_sep_unlabeled/nyt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
